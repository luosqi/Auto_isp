import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Tuple, Dict


class YOLOv3Loss(nn.Module):
    """
    YOLOv3æŸå¤±å‡½æ•°
    å¤ç°è®ºæ–‡ä¸­ä½¿ç”¨çš„æ£€æµ‹Lossï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±è®¡ç®—
    
    LossåŒ…å«ä¸‰éƒ¨åˆ†ï¼š
    1. å®šä½æŸå¤± (bbox regression)
    2. ç½®ä¿¡åº¦æŸå¤± (objectness)
    3. åˆ†ç±»æŸå¤± (class prediction)
    """
    def __init__(
        self,
        num_classes: int = 80,
        anchors: List[Tuple[int, int]] = None,
        img_size: int = 512,
        ignore_thresh: float = 0.5
    ):
        super().__init__()
        self.num_classes = num_classes
        self.img_size = img_size
        self.ignore_thresh = ignore_thresh
        
        # YOLOv3ä½¿ç”¨3ä¸ªå°ºåº¦ï¼Œæ¯ä¸ªå°ºåº¦3ä¸ªanchor
        if anchors is None:
            # COCO datasetçš„é»˜è®¤anchors (ç›¸å¯¹äº416x416)
            # æŒ‰å¤§å°æ’åºï¼šå° -> ä¸­ -> å¤§
            self.anchors = [
                [(10, 13), (16, 30), (33, 23)],      # å°ç›®æ ‡ (52x52)
                [(30, 61), (62, 45), (59, 119)],     # ä¸­ç›®æ ‡ (26x26)
                [(116, 90), (156, 198), (373, 326)]  # å¤§ç›®æ ‡ (13x13)
            ]
        else:
            self.anchors = anchors
        
        # ç¼©æ”¾åˆ°å½“å‰è¾“å…¥å°ºå¯¸
        self.scaled_anchors = []
        for scale_anchors in self.anchors:
            scaled = [(w * img_size / 416, h * img_size / 416) 
                     for w, h in scale_anchors]
            self.scaled_anchors.append(scaled)
        
        self.mse_loss = nn.MSELoss(reduction='sum')
        self.bce_loss = nn.BCELoss(reduction='sum')
        
    def forward(
        self, 
        predictions: List[torch.Tensor], 
        targets: Dict[str, torch.Tensor]
    ) -> torch.Tensor:
        """
        Args:
            predictions: List of 3 tensors, shapes:
                - (B, 3, 13, 13, 5+num_classes)  # å¤§ç›®æ ‡
                - (B, 3, 26, 26, 5+num_classes)  # ä¸­ç›®æ ‡
                - (B, 3, 52, 52, 5+num_classes)  # å°ç›®æ ‡
            targets: Dict with keys:
                - 'boxes': (B, N, 4) normalized [x1, y1, x2, y2]
                - 'labels': (B, N) class indices
                
        Returns:
            total_loss: scalar tensor
        """
        device = predictions[0].device
        batch_size = predictions[0].size(0)
        
        total_loss = torch.zeros(1, device=device)
        
        # éå†3ä¸ªå°ºåº¦
        for scale_idx, pred in enumerate(predictions):
            # pred shape: (B, 3, H, W, 5+C)
            B, num_anchors, grid_h, grid_w, _ = pred.shape
            
            # æ„å»ºè¯¥å°ºåº¦çš„targets
            scale_targets = self._build_targets(
                pred, targets, self.scaled_anchors[scale_idx], 
                grid_h, grid_w, scale_idx
            )
            
            # åˆ†ç¦»é¢„æµ‹å€¼
            pred_boxes = pred[..., :4]      # (B, 3, H, W, 4)
            pred_conf = pred[..., 4:5]      # (B, 3, H, W, 1)
            pred_cls = pred[..., 5:]        # (B, 3, H, W, C)
            
            # è·å–mask
            obj_mask = scale_targets['obj_mask']      # (B, 3, H, W, 1)
            noobj_mask = scale_targets['noobj_mask']  # (B, 3, H, W, 1)
            
            # 1. å®šä½æŸå¤± (åªè®¡ç®—æœ‰ç›®æ ‡çš„æ ¼å­)
            if obj_mask.sum() > 0:
                target_boxes = scale_targets['boxes']  # (B, 3, H, W, 4)
                
                # ä½¿ç”¨MSE Loss (è®ºæ–‡ä¸­çš„åšæ³•)
                box_loss = self.mse_loss(
                    pred_boxes[obj_mask.squeeze(-1)],
                    target_boxes[obj_mask.squeeze(-1)]
                )
                total_loss += box_loss
            
            # 2. ç½®ä¿¡åº¦æŸå¤±
            # 2a. æœ‰ç›®æ ‡çš„æ ¼å­ï¼šé¢„æµ‹ä¸º1
            if obj_mask.sum() > 0:
                conf_loss_obj = self.bce_loss(
                    pred_conf[obj_mask],
                    torch.ones_like(pred_conf[obj_mask])
                )
                total_loss += conf_loss_obj
            
            # 2b. æ— ç›®æ ‡çš„æ ¼å­ï¼šé¢„æµ‹ä¸º0
            if noobj_mask.sum() > 0:
                conf_loss_noobj = self.bce_loss(
                    pred_conf[noobj_mask],
                    torch.zeros_like(pred_conf[noobj_mask])
                )
                # æ— ç›®æ ‡æŸå¤±æƒé‡é™ä½ï¼ˆè®ºæ–‡ä¸­çš„å¤„ç†ï¼‰
                total_loss += 0.5 * conf_loss_noobj
            
            # 3. åˆ†ç±»æŸå¤± (åªè®¡ç®—æœ‰ç›®æ ‡çš„æ ¼å­)
            if obj_mask.sum() > 0:
                target_cls = scale_targets['classes']  # (B, 3, H, W, C)
                cls_loss = self.bce_loss(
                    pred_cls[obj_mask.squeeze(-1)],
                    target_cls[obj_mask.squeeze(-1)]
                )
                total_loss += cls_loss
        
        # å½’ä¸€åŒ– (é™¤ä»¥batch size)
        total_loss = total_loss / batch_size
        
        return total_loss
    
    def _build_targets(
        self,
        pred: torch.Tensor,
        targets: Dict[str, torch.Tensor],
        anchors: List[Tuple[float, float]],
        grid_h: int,
        grid_w: int,
        scale_idx: int
    ) -> Dict[str, torch.Tensor]:
        """
        æ„å»ºè¯¥å°ºåº¦çš„è®­ç»ƒç›®æ ‡
        
        Returns:
            targets_dict: Dict with keys:
                - 'obj_mask': (B, 3, H, W, 1) bool, æœ‰ç›®æ ‡çš„ä½ç½®
                - 'noobj_mask': (B, 3, H, W, 1) bool, æ— ç›®æ ‡çš„ä½ç½®
                - 'boxes': (B, 3, H, W, 4) ç›®æ ‡æ¡†åæ ‡
                - 'classes': (B, 3, H, W, C) one-hotç±»åˆ«
        """
        device = pred.device
        B = pred.size(0)
        num_anchors = len(anchors)
        
        # åˆå§‹åŒ–
        obj_mask = torch.zeros(B, num_anchors, grid_h, grid_w, 1, 
                              dtype=torch.bool, device=device)
        noobj_mask = torch.ones(B, num_anchors, grid_h, grid_w, 1,
                               dtype=torch.bool, device=device)
        
        target_boxes = torch.zeros(B, num_anchors, grid_h, grid_w, 4, device=device)
        target_cls = torch.zeros(B, num_anchors, grid_h, grid_w, self.num_classes, 
                                device=device)
        
        # å¤„ç†æ¯ä¸ªæ ·æœ¬
        for b in range(B):
            boxes = targets['boxes'][b]  # (N, 4) [x1, y1, x2, y2] normalized
            labels = targets['labels'][b]  # (N,)
            
            if len(boxes) == 0:
                continue
            
            # è½¬æ¢ä¸ºä¸­å¿ƒç‚¹æ ¼å¼
            boxes_cxcywh = self._xyxy_to_cxcywh(boxes)  # (N, 4) [cx, cy, w, h]
            
            # ç¼©æ”¾åˆ°gridå°ºå¯¸
            boxes_cxcywh[:, 0] *= grid_w
            boxes_cxcywh[:, 1] *= grid_h
            boxes_cxcywh[:, 2] *= self.img_size
            boxes_cxcywh[:, 3] *= self.img_size
            
            # ä¸ºæ¯ä¸ªGTæ¡†åˆ†é…anchor
            for box_idx, (box, label) in enumerate(zip(boxes_cxcywh, labels)):
                if label < 0:  # å¿½ç•¥æ ‡ç­¾
                    continue
                
                cx, cy, w, h = box
                
                # æ‰¾åˆ°è¯¥æ¡†æ‰€åœ¨çš„grid cell
                grid_x = int(cx)
                grid_y = int(cy)
                
                if grid_x >= grid_w or grid_y >= grid_h:
                    continue
                
                # è®¡ç®—ä¸æ‰€æœ‰anchorçš„IoUï¼Œé€‰æ‹©æœ€ä½³anchor
                anchor_ious = []
                for anchor_w, anchor_h in anchors:
                    iou = self._bbox_iou_wh(w, h, anchor_w, anchor_h)
                    anchor_ious.append(iou)
                
                best_anchor_idx = np.argmax(anchor_ious)
                
                # è®¾ç½®è¯¥ä½ç½®ä¸ºæœ‰ç›®æ ‡
                obj_mask[b, best_anchor_idx, grid_y, grid_x, 0] = True
                noobj_mask[b, best_anchor_idx, grid_y, grid_x, 0] = False
                
                # è®¾ç½®ç›®æ ‡å€¼
                # bbox: ç›¸å¯¹äºgrid cellçš„åç§»
                tx = cx - grid_x
                ty = cy - grid_y
                tw = torch.log(w / anchors[best_anchor_idx][0] + 1e-16)
                th = torch.log(h / anchors[best_anchor_idx][1] + 1e-16)
                
                target_boxes[b, best_anchor_idx, grid_y, grid_x] = \
                    torch.tensor([tx, ty, tw, th], device=device)
                
                # class: one-hot
                target_cls[b, best_anchor_idx, grid_y, grid_x, int(label)] = 1.0
        
        return {
            'obj_mask': obj_mask,
            'noobj_mask': noobj_mask,
            'boxes': target_boxes,
            'classes': target_cls
        }
    
    @staticmethod
    def _xyxy_to_cxcywh(boxes):
        """[x1,y1,x2,y2] -> [cx,cy,w,h]"""
        cx = (boxes[:, 0] + boxes[:, 2]) / 2
        cy = (boxes[:, 1] + boxes[:, 3]) / 2
        w = boxes[:, 2] - boxes[:, 0]
        h = boxes[:, 3] - boxes[:, 1]
        return torch.stack([cx, cy, w, h], dim=1)
    
    @staticmethod
    def _bbox_iou_wh(w1, h1, w2, h2):
        """è®¡ç®—ä¸¤ä¸ªæ¡†çš„IoU (ä»…åŸºäºå®½é«˜)"""
        inter_w = min(w1, w2)
        inter_h = min(h1, h2)
        inter_area = inter_w * inter_h
        
        union_area = w1 * h1 + w2 * h2 - inter_area
        
        return inter_area / (union_area + 1e-16)


class YOLOv3Detector(nn.Module):
    """
    YOLOv3æ£€æµ‹å™¨åŒ…è£…ç±»
    ç”¨äºRLè®­ç»ƒä¸­è®¡ç®—æ£€æµ‹loss
    """
    def __init__(
        self,
        num_classes: int = 80,
        img_size: int = 512,
        pretrained: bool = True,
        freeze_backbone: bool = True
    ):
        super().__init__()
        self.num_classes = num_classes
        self.img_size = img_size
        
        # åŠ è½½é¢„è®­ç»ƒçš„YOLOv3
        try:
            # å°è¯•ä½¿ç”¨ultralyticsçš„å®ç°
            from ultralytics import YOLO
            self.model = YOLO('yolov3.pt')
            self.use_ultralytics = True
            print("âœ… Loaded YOLOv3 from ultralytics")
        except:
            # å¤‡é€‰ï¼šä½¿ç”¨torchvisionçš„Faster R-CNNä½œä¸ºæ›¿ä»£
            print("âš ï¸ ultralytics not available, using Faster R-CNN as fallback")
            from torchvision.models.detection import fasterrcnn_resnet50_fpn
            self.model = fasterrcnn_resnet50_fpn(pretrained=True)
            self.use_ultralytics = False
        
        # å†»ç»“backboneï¼ˆè®ºæ–‡è¦æ±‚ï¼‰
        if freeze_backbone:
            for param in self.model.parameters():
                param.requires_grad = False
            print("ğŸ”’ YOLOv3 backbone frozen")
        
        self.model.eval()
        
        # Lossè®¡ç®—å™¨
        self.loss_fn = YOLOv3Loss(
            num_classes=num_classes,
            img_size=img_size
        )
    
    def forward(self, images: torch.Tensor, targets: Dict = None):
        """
        Args:
            images: (B, 3, H, W) RGBå›¾åƒ
            targets: è®­ç»ƒæ—¶çš„GTæ ‡æ³¨
            
        Returns:
            å¦‚æœtargets=None: è¿”å›é¢„æµ‹ç»“æœ
            å¦‚æœtargets!=None: è¿”å›loss
        """
        if targets is None:
            # æ¨ç†æ¨¡å¼
            with torch.no_grad():
                return self.model(images)
        else:
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—loss
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦è·å–ä¸­é—´ç‰¹å¾æ¥è®¡ç®—YOLO loss
            return self._compute_loss(images, targets)
    
    def _compute_loss(self, images, targets):
        """
        è®¡ç®—YOLOv3æ£€æµ‹loss
        
        æ³¨æ„ï¼šç”±äºä¸åŒYOLOå®ç°å·®å¼‚ï¼Œè¿™é‡Œæä¾›ç®€åŒ–ç‰ˆæœ¬
        å®é™…é¡¹ç›®ä¸­å»ºè®®ç›´æ¥ä½¿ç”¨ultralyticsçš„lossè®¡ç®—
        """
        if self.use_ultralytics:
            # ultralyticsçš„YOLOå¯ä»¥ç›´æ¥è®¡ç®—loss
            results = self.model.train()
            # éœ€è¦è°ƒç”¨æ¨¡å‹çš„lossè®¡ç®—æ¥å£
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šä½¿ç”¨æ¨¡å‹çš„forwardè¿”å›
            outputs = self.model.model(images)
            
            # æå–3ä¸ªå°ºåº¦çš„é¢„æµ‹
            # æ³¨æ„ï¼šå®é™…å®ç°éœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹ç»“æ„è°ƒæ•´
            predictions = self._extract_predictions(outputs)
            
            # è®¡ç®—loss
            loss = self.loss_fn(predictions, targets)
            return loss
        else:
            # ä½¿ç”¨Faster R-CNNçš„loss
            # è½¬æ¢targetsæ ¼å¼
            target_list = []
            for b in range(len(targets['boxes'])):
                target_list.append({
                    'boxes': targets['boxes'][b] * self.img_size,  # åå½’ä¸€åŒ–
                    'labels': targets['labels'][b]
                })
            
            loss_dict = self.model(images, target_list)
            # Faster R-CNNè¿”å›losså­—å…¸
            total_loss = sum(loss for loss in loss_dict.values())
            return total_loss
    
    def _extract_predictions(self, outputs):
        """
        ä»æ¨¡å‹è¾“å‡ºä¸­æå–3ä¸ªå°ºåº¦çš„é¢„æµ‹
        è¿™ä¸ªå‡½æ•°éœ€è¦æ ¹æ®å…·ä½“çš„YOLOå®ç°è°ƒæ•´
        """
        # å ä½å®ç°
        # å®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®åŠ è½½çš„æ¨¡å‹ç»“æ„ä¿®æ”¹
        if isinstance(outputs, (list, tuple)):
            return outputs[:3]  # å‡è®¾å‰3ä¸ªæ˜¯ä¸åŒå°ºåº¦
        else:
            # å¦‚æœæ˜¯å•ä¸ªtensorï¼Œéœ€è¦æ‰‹åŠ¨split
            # è¿™é‡Œè¿”å›ç®€åŒ–ç‰ˆæœ¬
            return [outputs, outputs, outputs]


# === ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥ä½¿ç”¨æ£€æµ‹ç»“æœä½œä¸ºloss ===
class SimpleDetectionLoss(nn.Module):
    """
    ç®€åŒ–çš„æ£€æµ‹lossè®¡ç®—
    å½“æ— æ³•è·å–YOLOv3å†…éƒ¨lossæ—¶ï¼Œä½¿ç”¨æ£€æµ‹ç»“æœåæ¨loss
    
    æ€è·¯ï¼šæ£€æµ‹æ•ˆæœè¶Šå¥½ -> lossè¶Šå° -> rewardè¶Šå¤§
    """
    def __init__(self, img_size=512):
        super().__init__()
        self.img_size = img_size
        
    def forward(self, predictions, targets):
        """
        Args:
            predictions: æ£€æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ ·æœ¬çš„æ£€æµ‹æ¡†
                æ ¼å¼: List[Dict] with keys 'boxes', 'scores', 'labels'
            targets: GTæ ‡æ³¨
            
        Returns:
            loss: åŸºäºæ£€æµ‹ç²¾åº¦è®¡ç®—çš„ä¼ªloss
        """
        device = targets['boxes'].device
        batch_size = targets['boxes'].size(0)
        
        total_loss = torch.zeros(1, device=device)
        
        for b in range(batch_size):
            gt_boxes = targets['boxes'][b]  # (N, 4)
            gt_labels = targets['labels'][b]  # (N,)
            
            if len(predictions) > b:
                pred_boxes = predictions[b]['boxes']  # (M, 4)
                pred_scores = predictions[b]['scores']  # (M,)
                pred_labels = predictions[b]['labels']  # (M,)
                
                # è®¡ç®—åŒ¹é…å¾—åˆ†
                if len(gt_boxes) > 0 and len(pred_boxes) > 0:
                    # è®¡ç®—IoUçŸ©é˜µ
                    ious = self._box_iou(pred_boxes, gt_boxes)
                    
                    # å¯¹æ¯ä¸ªGTï¼Œæ‰¾æœ€ä½³åŒ¹é…
                    max_ious, _ = ious.max(dim=0)
                    
                    # Loss = 1 - average_max_iou
                    # (IoUè¶Šé«˜ï¼Œlossè¶Šä½)
                    loss = 1.0 - max_ious.mean()
                else:
                    # æ²¡æœ‰é¢„æµ‹æˆ–æ²¡æœ‰GT
                    loss = torch.tensor(1.0, device=device)
            else:
                loss = torch.tensor(1.0, device=device)
            
            total_loss += loss
        
        return total_loss / batch_size
    
    @staticmethod
    def _box_iou(boxes1, boxes2):
        """è®¡ç®—IoUçŸ©é˜µ"""
        area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
        area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])
        
        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])
        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
        
        wh = (rb - lt).clamp(min=0)
        inter = wh[:, :, 0] * wh[:, :, 1]
        
        union = area1[:, None] + area2 - inter
        
        iou = inter / (union + 1e-6)
        return iou